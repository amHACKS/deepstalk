{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TestScraper for integrating LinkedIn in the future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipython in ./anaconda3/lib/python3.8/site-packages (7.16.1)\n",
      "Requirement already satisfied: pygments in ./anaconda3/lib/python3.8/site-packages (from ipython) (2.6.1)\n",
      "Requirement already satisfied: pickleshare in ./anaconda3/lib/python3.8/site-packages (from ipython) (0.7.5)\n",
      "Requirement already satisfied: jedi>=0.10 in ./anaconda3/lib/python3.8/site-packages (from ipython) (0.17.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in ./anaconda3/lib/python3.8/site-packages (from ipython) (49.2.0.post20200714)\n",
      "Requirement already satisfied: decorator in ./anaconda3/lib/python3.8/site-packages (from ipython) (4.4.2)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in ./anaconda3/lib/python3.8/site-packages (from ipython) (3.0.5)\n",
      "Requirement already satisfied: backcall in ./anaconda3/lib/python3.8/site-packages (from ipython) (0.2.0)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in ./anaconda3/lib/python3.8/site-packages (from ipython) (4.8.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in ./anaconda3/lib/python3.8/site-packages (from ipython) (4.3.3)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in ./anaconda3/lib/python3.8/site-packages (from jedi>=0.10->ipython) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./anaconda3/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython) (0.2.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./anaconda3/lib/python3.8/site-packages (from pexpect; sys_platform != \"win32\"->ipython) (0.6.0)\n",
      "Requirement already satisfied: six in ./anaconda3/lib/python3.8/site-packages (from traitlets>=4.2->ipython) (1.15.0)\n",
      "Requirement already satisfied: ipython-genutils in ./anaconda3/lib/python3.8/site-packages (from traitlets>=4.2->ipython) (0.2.0)\n",
      "Requirement already satisfied: selenium in ./anaconda3/lib/python3.8/site-packages (3.141.0)\n",
      "Requirement already satisfied: urllib3 in ./anaconda3/lib/python3.8/site-packages (from selenium) (1.25.9)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement time (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for time\u001b[0m\n",
      "Requirement already satisfied: parsel in ./anaconda3/lib/python3.8/site-packages (1.6.0)\n",
      "Requirement already satisfied: six>=1.6.0 in ./anaconda3/lib/python3.8/site-packages (from parsel) (1.15.0)\n",
      "Requirement already satisfied: cssselect>=0.9 in ./anaconda3/lib/python3.8/site-packages (from parsel) (1.1.0)\n",
      "Requirement already satisfied: w3lib>=1.19.0 in ./anaconda3/lib/python3.8/site-packages (from parsel) (1.22.0)\n",
      "Requirement already satisfied: lxml in ./anaconda3/lib/python3.8/site-packages (from parsel) (4.5.0)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement csv (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for csv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install ipython \n",
    "!pip3 install selenium  \n",
    "!pip3 install time \n",
    "!pip3 install parsel\n",
    "!pip3 install csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Needed imports ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy import OAuthHandler\n",
    "from tweepy.streaming import StreamListener\n",
    "import tweepy\n",
    "import json\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "import string\n",
    "import preprocessor as p\n",
    "import os\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter twitter access tokens ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = \"\"\n",
    "consumer_secret = \"\"\n",
    "access_key = \"\"\n",
    "access_secret = \"\"\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_key, access_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function for scraping tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraptweets(search_words, date_since, numTweets, numRuns):\n",
    "    db_tweets = pd.DataFrame(columns = ['username', 'acctdesc', 'location', 'following',\n",
    "                                        'followers', 'totaltweets', 'usercreatedts', 'tweetcreatedts',\n",
    "                                        'retweetcount', 'text', 'hashtags']\n",
    "                                )\n",
    "    program_start = time.time()\n",
    "    for i in range(0,numRuns):\n",
    "        start_run = time.time()\n",
    "        tweets = tweepy.Cursor(api.search, q=search_words, lang=\"en\", since=date_since, tweet_mode='extended').items(numTweets)\n",
    "        tweet_list = [tweet for tweet in tweets]\n",
    "        noTweets = 0\n",
    "        for tweet in tweet_list:\n",
    "            username = tweet.user.screen_name\n",
    "            acctdesc = tweet.user.description\n",
    "            location = tweet.user.location\n",
    "            following = tweet.user.friends_count\n",
    "            followers = tweet.user.followers_count\n",
    "            totaltweets = tweet.user.statuses_count\n",
    "            usercreatedts = tweet.user.created_at\n",
    "            tweetcreatedts = tweet.created_at\n",
    "            retweetcount = tweet.retweet_count\n",
    "            hashtags = tweet.entities['hashtags']\n",
    "            try:\n",
    "                text = tweet.retweeted_status.full_text\n",
    "            except AttributeError:  \n",
    "                text = tweet.full_text\n",
    "                ith_tweet = [username, acctdesc, location, following, followers, totaltweets,\n",
    "                         usercreatedts, tweetcreatedts, retweetcount, text, hashtags]\n",
    "                \n",
    "                db_tweets.loc[len(db_tweets)] = ith_tweet\n",
    "                noTweets+=1\n",
    "        end_run = time.time()\n",
    "        duration_run = round((end_run-start_run)/60, 2)\n",
    "        \n",
    "        print('no. of tweets scraped for run {} is {}'.format(i + 1, noTweets))\n",
    "        print('time take for {} run to complete is {} mins'.format(i+1, duration_run))\n",
    "        \n",
    "        time.sleep(920)\n",
    "        \n",
    "    \n",
    "    print(db_tweets.head())\n",
    "    \n",
    "    program_end = time.time()\n",
    "    print('Scraping has completed!')\n",
    "    print('Total time taken to scrap is {} minutes.'.format(round(program_end - program_start)/60, 2))\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_words = \"#hongkong OR #hkprotests OR #freehongkong OR #hongkongprotests OR #hkpolicebrutality OR #antichinazi OR #standwithhongkong OR #hkpolicestate OR #HKpoliceterrorist OR #standwithhk OR #hkpoliceterrorism\"\n",
    "date_since = \"2019-11-03\"\n",
    "numTweets = 2500\n",
    "numRuns = 1# Call the function scraptweets\n",
    "scraptweets(search_words, date_since, numTweets, numRuns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LinkedIn Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linkedin_scraper import Person, actions\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "driver = webdriver.Chrome('/home/dazedtiara6667/.wdm/drivers/chromedriver/linux64/88.0.4324.96/chromedriver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = \"rishab-mudliar-8022171b1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "email = \"\"\n",
    "password = \"\"\n",
    "actions.login(driver, email, password)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linkedin_scraper import Person\n",
    "person1 = Person('https://www.linkedin.com/in/%s'%username, driver = driver, scrape=False)\n",
    "person1.scrape()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping user details after logging in to linkedin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Rishab Mudliar\n",
      "Experience: Professional Basketball Player at National Basketball Association (NBA) from 2004 – to  for 17 yrs based at United States\n",
      "Experience: Venture Partner at None from None to None for None based at None\n",
      "Experience: Investment Committee at None from None to None for None based at None\n",
      "Experience: Board Member at None from None to None for None based at None\n",
      "Experience: Vice President at National Basketball Players Association (NBPA) from Feb 2013 to Present for 8 yrs 1 mo based at New York, NY\n",
      "Experience: Professional Basketball Player at National Basketball Association (NBA) from 2004 – to  for 17 yrs based at United States\n",
      "Experience: Venture Partner at None from None to None for None based at None\n",
      "Experience: Investment Committee at None from None to None for None based at None\n",
      "Experience: Board Member at None from None to None for None based at None\n",
      "Experience: Vice President at National Basketball Players Association (NBPA) from Feb 2013 to Present for 8 yrs 1 mo based at New York, NY\n",
      "Experience: Professional Basketball Player at National Basketball Association (NBA) from 2004 – to  for 17 yrs based at United States\n",
      "Experience: Venture Partner at None from None to None for None based at None\n",
      "Experience: Investment Committee at None from None to None for None based at None\n",
      "Experience: Board Member at None from None to None for None based at None\n",
      "Experience: Vice President at National Basketball Players Association (NBPA) from Feb 2013 to Present for 8 yrs 1 mo based at New York, NY\n",
      "Experience: Professional Basketball Player at National Basketball Association (NBA) from 2004 – to  for 17 yrs based at United States\n",
      "Experience: Venture Partner at None from None to None for None based at None\n",
      "Experience: Investment Committee at None from None to None for None based at None\n",
      "Experience: Board Member at None from None to None for None based at None\n",
      "Experience: Vice President at National Basketball Players Association (NBPA) from Feb 2013 to Present for 8 yrs 1 mo based at New York, NY\n",
      "Experience: Member at None from None to None for None based at None\n",
      "Experience: Member at None from None to None for None based at None\n",
      "Experience: Member at None from None to None for None based at None\n",
      "Experience: Member at None from None to None for None based at None\n",
      "Experience: Member at None from None to None for None based at None\n",
      "Education: None at University of Arizona from None to None\n",
      "Education: None at University of Arizona from None to None\n",
      "Education: None at University of Arizona from None to None\n",
      "Education: None at University of Arizona from None to None\n",
      "Education: Bachelor of Technology - BTech at AMRITA VISHWA VIDYAPEETHAM from 2019 to 2023\n",
      "Education: Bachelor of Technology - BTech at AMRITA VISHWA VIDYAPEETHAM from 2019 to 2023\n",
      "Education: Bachelor of Technology - BTech at AMRITA VISHWA VIDYAPEETHAM from 2019 to 2023\n",
      "Education: Bachelor of Technology - BTech at AMRITA VISHWA VIDYAPEETHAM from 2019 to 2023\n",
      "Education: Bachelor of Technology - BTech at AMRITA VISHWA VIDYAPEETHAM from 2019 to 2023\n",
      "https://www.linkedin.com/in/rishab-mudliar-8022171b1\n",
      "National Basketball Association (NBA)\n"
     ]
    }
   ],
   "source": [
    "print(\"Name: \" + person1.name)\n",
    "for experience in person1.experiences :\n",
    "    print(\"Experience: \"+ str(experience))\n",
    "for education in person1.educations :\n",
    "    print(\"Education: \" + str(education))\n",
    "for interest in person1.interests:\n",
    "    print(\"Interest: \"+ str(interest))\n",
    "for accomplishment in person1.accomplishments:\n",
    "    print(\"Accomplishment: \"+ str(accomplishment))\n",
    "print (person1.linkedin_url)\n",
    "print(person1.company)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another way to scrape LinkedIn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Rishab Mudliar\n",
      "Job Title: Member at FOSS@Amrita | IoT | Problem Solving | Android(Amateur)\n",
      "LinkedIn URL: https://www.linkedin.com/in/rishab-mudliar-8022171b1/\n",
      "Company: FOSS@Amrita\n",
      "Location: Nagpur, Maharashtra, India\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "from parsel import Selector\n",
    "# xpath to extract the text from the class containing the name\n",
    "sel = Selector(text=driver.page_source) \n",
    "name = sel.xpath('//*[starts-with(@class, \"inline t-24 t-black t-normal break-words\")]/text()').extract_first()\n",
    "\n",
    "if name:\n",
    "    name = name.strip()\n",
    "\n",
    "job_title = sel.xpath('//*[starts-with(@class, \"mt1 t-18 t-black t-normal break-words\")]/text()').extract_first()\n",
    "\n",
    "if job_title:\n",
    "    job_title = job_title.strip()\n",
    "    \n",
    "company = sel.xpath('//*[starts-with(@class, \"text-align-left ml2 t-14 t-black t-bold full-width lt-line-clamp lt-line-clamp--multi-line ember-view\")]/text()').extract_first()\n",
    "\n",
    "if company:\n",
    "    company = company.strip()\n",
    "\n",
    "\n",
    "\n",
    "# xpath to extract the text from the class containing the location\n",
    "location = sel.xpath('//*[starts-with(@class, \"t-16 t-black t-normal inline-block\")]/text()').extract_first()\n",
    "\n",
    "if location:\n",
    "    location = location.strip()\n",
    "    \n",
    "linkedin_url = driver.current_url\n",
    "\n",
    "print(\"Name: \" + name)\n",
    "print(\"Job Title: \"+ job_title)\n",
    "print(\"LinkedIn URL: \"+ linkedin_url)\n",
    "print(\"Company: \" + company)\n",
    "print(\"Location: \"+ location)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
